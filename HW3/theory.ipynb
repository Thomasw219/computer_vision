{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025d6187",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc25b0a4ba6f32aecb61b1c243e16de3",
     "grade": false,
     "grade_id": "cell-63d8d04a820a9a31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Neural Networks for Recognition - Assignment 3\n",
    "\n",
    "     Instructor: Kris Kitani                       TAs: Qichen(Lead), Paritosh, Rawal, Yan, Zen, Wen-Hsuan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e0b61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0598b3ff2a94d3ff38afa7b1c21e1c07",
     "grade": false,
     "grade_id": "cell-f9a9cc792e95e7c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Submission Instructions:\n",
    "\n",
    "1. Submit the PDF version of `theory.ipynb` to HW3:PDF. The `theory.ipynb` should include **ALL the writeup answers AND ALL the screenshots of code** specifically required in questions **from Q1 to Q7**. This section will be manually Graded.\n",
    "\n",
    "2. Submit `q2.ipynb`, `q3.ipynb`, `q5.ipynb` to HW3:Code. Please do not submit other jupyter notebooks as they will not be autograded. Submitting them may cause running time out. (`q5.ipynb` is optional for extra credits)\n",
    "\n",
    "**The Appendix section at the end of this file would help you on questions P1 and P2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee6744",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "544429557fd95d21890d2229c74a7ff4",
     "grade": false,
     "grade_id": "cell-e4414269f8c1645b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1 Theory Questions  (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab18847",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d8aa00056b3211954c375ea35e715b2",
     "grade": false,
     "grade_id": "cell-cf1227140b9ba295",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.1 (4 Points WriteUp)\n",
    "Prove that softmax is invariant to translation, that is \n",
    "$$softmax(x) = softmax(x + c) \\qquad \\forall c \\in \\mathbb{R}$$\n",
    "Softmax is defined as below, for each index $i$ in a vector $x$.\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n",
    "Often we use $c = − \\max x_i$. Why is that a good idea? (Tip: consider the range of values that numerator will have with $c = 0$ and $c = − \\max x_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22802e66",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3941e91facfc23f3fe531c8367d945e8",
     "grade": true,
     "grade_id": "cell-974a0ae8f06d6586",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "In order to prove the above, we simply evaluate the function after translation\n",
    "$$softmax(x_i + c) = \\frac{e^{x_i + c}}{\\sum_j e^{x_j + c}} = \\frac{e^{x_i}e^c}{\\sum_j e^{x_j}e^c} = \\frac{e^{x_i}e^c}{e^c\\sum_j e^{x_j}} = \\frac{e^{x_i}}{\\sum_j e^{x_j}} = softmax(x_i)$$\n",
    "With $c = -\\max x_i$ we have that $e^{x_i + c} \\in [0, 1]$. Wheras, if $c = 0$ then the value of $e^{x_i}$ can be arbitrarily large. If some $e^{x_i}$ are very large and other $e^{x_j}$ are very small, a loss of precision can occur easily occur when computing the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b144ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e0ddcad64e1974a1b283ab4b960f3b8",
     "grade": false,
     "grade_id": "cell-948036aa5862be04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.2\n",
    "\n",
    "Softmax can be written as a three step processes, with $s_i = e^{x_i}$ , $S=\\sum_i s_i$ and $softmax(x_i)= \\frac{1}{S} s_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361cb29",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c9340fc4070513afbb444b1d1563eed",
     "grade": false,
     "grade_id": "cell-52cff4a1493a7fff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.1 (1 point WriteUp)\n",
    "As $x \\in \\mathbb{R}^d$, what are the properties of $softmax(x)$, namely what is the range of each element? What is the sum over all elements?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83138ae7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67459c94158ef00d66c43c9896caf245",
     "grade": true,
     "grade_id": "cell-56521164bb7b5976",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The range of each $softmax(x_i)$ is $[0,1]$ and the sum of the elements is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f3cff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28faddaa495b54e962d322d74f319c7a",
     "grade": false,
     "grade_id": "cell-3a8e1905906ce40a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.2 (1 point WriteUp)\n",
    "One could say that ”softmax takes an arbitrary real valued vector $x$ and turns it into a ___”. Please think about a short phrase to fill in ___."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b6fa2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13459335a8f97ab284dfde9c24523418",
     "grade": true,
     "grade_id": "cell-6044a09dfc72a819",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Probability distribution over a discrete number of outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec2b12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ac41c022a0f310b880438d66a81126e",
     "grade": false,
     "grade_id": "cell-501239c17b8d9fc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.3 (1 point WriteUp)\n",
    "Can you see the role of each step in the multi-step process now? Explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974cc8b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7279aaddce308bb0f897a9dd5e9d6ab",
     "grade": true,
     "grade_id": "cell-93d39e169a1584b1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The first step $s_i = e^{x_i}$ transforms the vector $x$ so all its elements are non-negative, the second step $S=\\sum_i s_i$ computes a normalization value so the sum of all the outputs will be 1, and $softmax(x_i)= \\frac{1}{S} s_i$ gives the probability mass for a particular $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381cd7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f0139c980e6271bdcf924a4c4a09cca",
     "grade": false,
     "grade_id": "cell-182ef4991d435a9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.3 (3 points WriteUp)\n",
    "Show that multi-layer neural networks without a non-linear activation function are equivalent to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a3792",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "152725c4c687af9a3cbc311a9bf689e8",
     "grade": true,
     "grade_id": "cell-31c734ab09fd4344",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The fully connected neural network operation without a non-linearity can by represented y = Wx + b.\n",
    "Two of these operations in succession can be written $y = W_2(W_1 x + b_1) + b_2$. We observe that this can be rewritten $y = W_2W_1x + W_2b_1 + b_2$ where $W_2W_1$ is just another matrix and $W_2b_1 + b_2$ is a column vector. Therefore the two successive operations can be done in one operation $y = Wx + b$ where $W = W_1W_2, b = W_2b_1 + b_2$. We can continue with induction to show that any number of layers without a non-linearity can just be represented in the form y = Wx + b. For any particular $y_i$ this is the same as linear regression with weights in row $i$ of matrix $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce15f8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c65af3aee0fcba80878ceaa805e2c911",
     "grade": false,
     "grade_id": "cell-4e4a1870feae2779",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.4 (4 points WriteUp) \n",
    "Given the sigmoid activation function $\\sigma(x) = \\frac{1}{1+e^{-x}}$ , derive the gradient of the sigmoid function and show that it can be written as a function of $\\sigma(x)$ (without having access to $x$ directly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36bc4e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d7be6df91900e029bbd3c1e75e45be6",
     "grade": true,
     "grade_id": "cell-bb3ef4dae24f0472",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "We first compute the derivative of $\\sigma(x)$.\n",
    "$$ \\sigma'(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\sigma(x)\\frac{e^{-x}}{1 + e^{-x}} = \\sigma(x)\\frac{e^{-x}}{e^{-x}(e^x + 1)} = \\sigma(x)\\frac{1}{(e^x + 1)} = \\sigma(x)\\sigma(-x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96ea82",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3dff66146a32c6b75e3b0d8a1dc75740",
     "grade": false,
     "grade_id": "cell-c77fa30dd0533616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.5 (12 points WriteUp)\n",
    "\n",
    "Given $y = W x + b$ (or $y_j = \\sum_{i=1}^d  x_{i} W_{ji} + b_j$), and the gradient of some loss $J$ with respect $y$, show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$. Be sure to do the derivatives with scalars and re-form the matrix form afterwards. Here are some notional suggestions.\n",
    "$$ \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{k \\times 1} \\quad W \\in \\mathbb{R}^{k \\times d} \\quad x \\in \\mathbb{R}^{d \\times 1} \\quad b \\in \\mathbb{R}^{k \\times 1}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb05c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b66294c9dda4ab13c5d027e6bc82e64",
     "grade": true,
     "grade_id": "cell-4dbc318b383bf70c",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "We know from the chain rule $\\frac{\\partial J}{\\partial W} = \\frac{\\partial J}{\\partial y}\\frac{\\partial y}{\\partial W}$. We know $y_j = \\sum_{i=1}^d  x_{i} W_{ji} + b_j$, allowing us to compute $\\frac{\\partial y_j}{\\partial W_{ik}}$; this will be $x_k$ if $i = j$ and 0 otherwise. This means that $\\frac{\\partial y_j}{\\partial W}$ is a matrix that is all 0 except for column $j$ which is equal to $x$ (dimensions $d \\times k$). Then we can compose $\\frac{\\partial y}{\\partial W}$ by appending $\\frac{\\partial y_j}{\\partial W}$ for all $j$ along an additional dimension. This will be a $k \\times d \\times k$ tensor. Without knowing $J$ specifically, we cannot derive $\\frac{\\partial J}{\\partial y}$ so we will just use $\\frac{\\partial J}{\\partial y} = \\delta$. Therefore we have $\\frac{\\partial J}{\\partial W} = \\delta \\frac{\\partial y}{\\partial W}$ where $\\frac{\\partial y}{\\partial W}$ is as found before.\n",
    "\n",
    "\n",
    "Again with the chain rule we have $\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}\\frac{\\partial y}{\\partial x}$. Let us look at $y_j = \\sum_{i=1}^d  x_{i} W_{ji} + b_j$ but now take the derivative with respect to $x$. We can see that this derivative is the vector $W_j$. From there we can assemble $\\frac{\\partial y}{\\partial x}$ by stacking $\\frac{\\partial y_j}{\\partial x} = W_j$ which gives us the original matrix $W$. Then we have $\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}W = \\delta W$\n",
    "\n",
    "\n",
    "Finally we use the chain rule to get $\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial y}\\frac{\\partial y}{\\partial b}$. Then for every $\\frac{\\partial y_j}{\\partial b_i}$ we can see that it is equal to $1$ if $i = j$, otherwise 0. Then to get $\\frac{\\partial y_j}{\\partial b}$ we compose $\\frac{\\partial y_j}{\\partial b_i}$ into a vector of length $k$. Then all these $\\frac{\\partial y_j}{\\partial b}$ are stacked into a matrix of size $k \\times k$ to get $\\frac{\\partial y}{\\partial b}$. We can see that this matrix is diagonal, in fact $I_{k \\times k}$. This gives us $\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial y}I_{k \\times k} =\\delta I_{k \\times k} = \\delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ea3cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2815b900c97a10748f67524bf51b533a",
     "grade": false,
     "grade_id": "cell-920e213ee3ea40e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.6 (15 points WriteUp)\n",
    "\n",
    "We will find the derivatives for Conv layers now. Since most Deep Learning frameworks such as Pytorch, Tensorflow use cross-correlation in their respective \"convolution\" functions ([Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/convolution)), we will continue this abuse of notation. So the operation performed with the Conv Layer weights will be cross-correlation.\n",
    "    \n",
    "The input, $x$ is of shape $M\\times N$ with C channels. This will be *convolved* (actually cross-correlation) with $D$ number of $K\\times K$ filters, each with a bias term. The stride is 1 and there will be no padding. We know the gradient of some loss $J$ with respect to the output $y$, which will have $D$ channels. Show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$.\n",
    "\n",
    "The dimensions and notation are as follows:\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{D\\times M_o \\times N_o}\n",
    "    \\quad\n",
    "    M_o = M-K+1\n",
    "    \\quad\n",
    "    N_o = N-K+1\n",
    "$$\n",
    "$$\n",
    "    x \\in \\mathbb{R}^{C\\times M \\times N}\n",
    "    \\quad\n",
    "    W \\in \\mathbb{R}^{D\\times C \\times K \\times K}\n",
    "    \\quad\n",
    "    b \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "$x_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the input\n",
    "\n",
    "$y_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the output\n",
    "\n",
    "$W_{d, c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column, the $c^{th}$ channel of the kernel of the $d^{th}$ filter\n",
    "\n",
    "*For this question, you may compute the derivatives with scalars only. You don't need to re-form the matrix*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bc5ff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d306d84ffe6763436b897502147ab18",
     "grade": true,
     "grade_id": "cell-84844fb51fed1aab",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "We know from the chain rule that $\\frac{\\partial J}{\\partial W} = \\frac{\\partial J}{\\partial y}\\frac{\\partial y}{\\partial W}$. We first seek to compute $\\frac{\\partial y}{\\partial W_{d, c, i, j}}$. We can write $y_{d, i, j} = b_d + \\sum_{c=1}^C \\sum_{i'=1}^{K} \\sum_{j'=1}^{K} x_{c, i + i' - 1, j + j' - 1} W_{d, c, i', j'}$. From here we can see that $\\frac{\\partial y_{d, i, j}}{\\partial W_{d, c, n, m}}$ is equal to $x_{c, i + n - 1, j + m - 1}$.\n",
    "\n",
    "From the chain rule we have that $\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y}\\frac{\\partial y}{\\partial x}$. Again we look to our expression $y = b_d + \\sum_{c=1}^C \\sum_{i'=1}^{K} \\sum_{j'=1}^{K} x_{c, i + i' - 1, j + j' - 1} W_{d, c, i', j'}$. We see that $\\frac{\\partial y_{d, i, j}}{\\partial x_{c, n, m}}$ is equal to $W_{d, c, n - i + 1, m - j + 1}$ if the indices $n - i + 1, m - j + 1 \\in [1, K]$ otherwise the value is 0.\n",
    "\n",
    "Once more from the chain rule we have $\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial y}\\frac{\\partial y}{\\partial b}$. Again we look to our expression $y = b_d + \\sum_{c=1}^C \\sum_{i'=1}^{K} \\sum_{j'=1}^{K} x_{c, i + i' - 1, j + j' - 1} W_{d, c, i', j'}$. We see that $\\frac{\\partial y_{d, i, j}}{\\partial b_{d'}}$ is equal to $1$ if $d=d'$ else it is equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fabf4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "674f5af48cfacd5f2b47e7085dba8ae3",
     "grade": false,
     "grade_id": "cell-1e10040668b36463",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.7\n",
    "\n",
    "When the neural network applies the elementwise activation function (such as sigmoid), the gradient of the activation function scales the back-propagation update. This is directly from the chain rule, $\\frac{d}{d x} f(g(x)) = f'(g(x)) g'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275b3c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7216e978dab46e7da90591e8b5eadcd4",
     "grade": false,
     "grade_id": "cell-d0948d91db6d6a34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.1 (1 point WriteUp)\n",
    "Consider the sigmoid activation function for deep neural networks. Why might it lead to a \"vanishing gradient\" problem if it is used for many layers (consider plotting the $\\sigma'(x)$ in Q1.4)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b71dcaba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d20b744c87c16cff3860cb71abd8b5b7",
     "grade": true,
     "grade_id": "cell-d89dec7f9dd5635d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x))\n",
    "\n",
    "x = np.linspace(-4, 4, num=200)\n",
    "plt.plot(x, sigmoid(x) * sigmoid(-1 * x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc93510f",
   "metadata": {},
   "source": [
    "After graphing the derivative of the sigmoid function, we see that its values are capped at 0.25, and in fact is far less than 0.25 for most of its domain. The deeper the network, the more sigmoid derivatives are multiplied in the chain rule computation, which can result in gradients that become extremely small. This is the vanishing gradient problem, because $x^n, x \\in [0, 0.25]$ diminishes exponentially with $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78443e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f84086fd60884f1666a86fc21b619897",
     "grade": false,
     "grade_id": "cell-ebcafd1b185b5253",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.2 (1 point WriteUp)\n",
    "Often it is replaced with $\\tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$. What are the output ranges of both $\\tanh$ and sigmoid? Why might we prefer $\\tanh$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940324a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c69505610f75e170201ac5810030f51",
     "grade": true,
     "grade_id": "cell-6ef0bff01cadddd2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The output range of the $\\tanh$ function is -1 to 1. This is double the range of the sigmoid function with is from 0 to 1. We might prefer the $\\tanh$ function because of the following question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa26b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c045a69bc896bfc119effaeac4fc27fa",
     "grade": false,
     "grade_id": "cell-210ca940cc6cf12f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.3 (1 point WriteUp)\n",
    "Why does $\\tanh(x)$ have less of a vanishing gradient problem? (plotting the derivatives helps! for reference: $\\tanh'(x) = 1 - \\tanh(x)^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f2de666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-4, 4, num=200)\n",
    "plt.plot(x, 1 - np.tanh(x)**2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03414d7f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67a3435ba44fcec113d0dd7c0a0f8013",
     "grade": true,
     "grade_id": "cell-8925f054fa7aeede",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The effect of this is in the magnitude of its derivative which has a range of 0 to 1 for the $\\tanh$ function and only 0 to 0.25 for the sigmoid function. Therefore the gradient vanishes at a slower rate with the use of the tanh function compared to the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24221ba8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c739863fc4ff1b97f9a5f3903ddd925",
     "grade": false,
     "grade_id": "cell-b22c9cf732ba7c0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.7.4 (1 point WriteUp)\n",
    "$\\tanh$ is a scaled and shifted version of the sigmoid. Show how $\\tanh(x)$ can be written in terms of $\\sigma(x)$. (*Hint: consider how to make it have the same range*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e1acfd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c71d5c8cbe29d4cf8846bb0508e636f",
     "grade": true,
     "grade_id": "cell-51d748c739cc28d1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$\\frac{\\tanh(\\frac{x}{2}) + 1}{2} = \\frac{1}{2}(\\frac{1-e^{-2\\frac{x}{2}}}{1+e^{-2\\frac{x}{2}}} + \\frac{1+e^{-2\\frac{x}{2}}}{1+e^{-2\\frac{x}{2}}}) = \\frac{1}{2}(\\frac{2}{1+e^{-x}}) = \\frac{1}{1+e^{-x}} = sigmoid(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48adfc1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d784dc196042141a43dd4d3ca1e02b1f",
     "grade": false,
     "grade_id": "cell-572b479fed175027",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "## For the following questions, please find the instructions in the corresponding jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd695937",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c91869b98f9b4caae54948f1b46215e6",
     "grade": false,
     "grade_id": "cell-73a9363dbb4eeb8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q2 Implement a Fully Connected Network (65 points + 10 Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f107feb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ed7a931a26950507cb91fe9f3a20e74",
     "grade": false,
     "grade_id": "cell-61d6980ffe69f0cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.1.1 (3 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da6806",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "481f81209d087fc42c585a066c0d38b9",
     "grade": true,
     "grade_id": "cell-5adbb50e2e048b6c",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Depending on the final activation function it could be 0.5 if sigmoid or $\\frac{1}{n}$ for all categories if softmax. If there is no final activation function it would just be all 0s. It might not be a good idea to start with this initialization, because for each layer, the gradients for the weights will all be the same. There will be no differentiation among the neurons in the hidden layers and the features they extract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d8a04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37da28bf7e7b703f770662d1b369983d",
     "grade": false,
     "grade_id": "cell-377494940d117ac8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.1.3 (2 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30968fff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea48c62fb88f8e96cfcca6b4750590f6",
     "grade": true,
     "grade_id": "cell-cd2436d1298e6db3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c715e54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5dab96c7da53901d6339a2edd564f38",
     "grade": false,
     "grade_id": "cell-e4b82a4d357cc644",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3 Training Models (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90a554",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c94c62abd3795b9cdd129d1a7bdd950",
     "grade": false,
     "grade_id": "cell-6489cea69bd8bdd3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.2 (3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63ff42",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e29bc06161593b30c7c37dd27ea29eba",
     "grade": true,
     "grade_id": "cell-e380768cc8511806",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72841ff4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3478e43dd1a1e0d9e798ec5ca6bfe1f",
     "grade": false,
     "grade_id": "cell-9a360d4e2d2a59f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.3 (2 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371db3f9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b855fc6e16d07858a3ab4b060d605316",
     "grade": true,
     "grade_id": "cell-7cf3247bb27e0005",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8e037",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82b24e4243fdda57695057b7d4ce081d",
     "grade": false,
     "grade_id": "cell-c10943ecdbefccf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.4 (3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bcb12",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d43991574656ba1a0eb1406923f56f2",
     "grade": true,
     "grade_id": "cell-d318551e7bb2f699",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e52d12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "388f963a9bb49f6382837fcc8ee45a96",
     "grade": false,
     "grade_id": "cell-9d5e6606a4d1e707",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3.5 (4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e2a03",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3f8d3fe3399fe704eacaf73425da961",
     "grade": true,
     "grade_id": "cell-699c18952c5f0ecf",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd606c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48a7c17fe06eaa41b58095991a3cc574",
     "grade": false,
     "grade_id": "cell-234eebae3fbb6b14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4 Extract Text from Images (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b1bf3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6605f95b8ece1b3f46f79b915dbe13b",
     "grade": false,
     "grade_id": "cell-564cb15873d70616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.1 (3 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed9080",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e08a4d82b841f978434652fadd620ac4",
     "grade": true,
     "grade_id": "cell-371a94429864d47b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8c60d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4edb60ed50aae686c4440d4da43c7203",
     "grade": false,
     "grade_id": "cell-466d4c77e800b085",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.2 (13 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36408ef9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "527abea6466e5fad42c1f2d89dbb31ac",
     "grade": true,
     "grade_id": "cell-30d1533c7f070694",
     "locked": false,
     "points": 13,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6842b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "167e6bb80f063a680466df23c8ed3a27",
     "grade": false,
     "grade_id": "cell-a4ab802715e8fde6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.3 (6 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60411b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "876f9ca6285b996ddfdfa544123ec302",
     "grade": true,
     "grade_id": "cell-d4a44957464f69b9",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09831f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3addd5c49e15d1c3a4db909e988d4cfa",
     "grade": false,
     "grade_id": "cell-2a94753cc27fccbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.4 (13 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c186c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9abbef4396c44e65c1bd963d38631dbe",
     "grade": true,
     "grade_id": "cell-737f94592ee1fc0c",
     "locked": false,
     "points": 13,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42d8cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb22c308c16293123a8cb36258be42",
     "grade": false,
     "grade_id": "cell-e5cd95eec95a2803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q5 Image Compression with Autoencoders [Extra Credit](25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212a33d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f785be92e6500cf812623c5f04def06e",
     "grade": false,
     "grade_id": "cell-8a21593a43945ffa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q5.1.1 [Extra Credit](10 points Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa1141",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d02ee49d26e0c7f7ec983ff8ed361c83",
     "grade": true,
     "grade_id": "cell-a14d388df159b04c",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5960d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "026c5dbdb601cd578f75ee1cf2370d98",
     "grade": false,
     "grade_id": "cell-6966f08a72f45ffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5.2 [Extra Credit](3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b5aa5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86cc58520dc956edf4e6ce2e6e7bbb97",
     "grade": true,
     "grade_id": "cell-a372ab8ed75b12ef",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f314e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a154ac361c62555f562ce6ead2afced",
     "grade": false,
     "grade_id": "cell-70958da3fad26122",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q5.3.1 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a5c8a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "098d955e7a2b27bd750f607b22649f8f",
     "grade": true,
     "grade_id": "cell-7fa95d191289b087",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edfab0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d821fa3a0e3d33936ed51e80b3b53437",
     "grade": false,
     "grade_id": "cell-ef6f9cc6c4e94fa5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q5.3.2 [Extra Credit](3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2611e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d51a06c464b2a3e64c918cd011f4c07",
     "grade": true,
     "grade_id": "cell-6ec226a41e5e9ec2",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde62bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a643c6ea6bd559bcd4227e73b7fc6004",
     "grade": false,
     "grade_id": "cell-7d5e7253cd0fa4de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q6 Comparing against PCA [Extra Credit](15 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0870e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3acfc281c735af9489846f6a15d52b80",
     "grade": false,
     "grade_id": "cell-b9da236a37a91e2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.1 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf78efd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ad8b69f5e20d8ca53ad73e0ecd2dac3",
     "grade": true,
     "grade_id": "cell-9454330fc70ecd95",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ab825",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3369f26cb22d40444e83b4e1249201d3",
     "grade": false,
     "grade_id": "cell-1b7ec6b69ff5f438",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.2 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee32d677",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e4f860c3b38646a09b8033ae6316174",
     "grade": true,
     "grade_id": "cell-95b567b5c8381e3b",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b74f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f31d56d2e923ff65f46531901d89801",
     "grade": false,
     "grade_id": "cell-78bf5924af955143",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.3 [Extra Credit](4 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91542536",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46fcbf9e5231f598b759b7c34f4d30d2",
     "grade": true,
     "grade_id": "cell-24b71dedcffa1338",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfbac3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4e3df385828fa4f76edf8c4b0116b92",
     "grade": false,
     "grade_id": "cell-2f4c2ac42d375426",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.4 [Extra Credit](3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215afb7c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18577ce2a29bc7dedc1f08a41a1cf78c",
     "grade": true,
     "grade_id": "cell-27922e2c1160421c",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a97ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "896262237ed588dd156db42e7e06c08a",
     "grade": false,
     "grade_id": "cell-8fac1ca3ff947951",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q7 PyTorch (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798781ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce78f233d97e1f4541527a039a629fa1",
     "grade": false,
     "grade_id": "cell-03dd8625c09f576b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.1 (10 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2e507",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e86a489666ac68fabc4f8d8bf152b612",
     "grade": true,
     "grade_id": "cell-5ffc6cb938d61d80",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c88cf1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69eafedd527ad9c3a8f669acddb315e4",
     "grade": false,
     "grade_id": "cell-b310f2ddba0f6647",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.2 (3 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e539c1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cacd397b428d4eadd73ea230f56d108",
     "grade": true,
     "grade_id": "cell-f5a9affcedad4b2e",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac5571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10e4fe6fb3561465bf1d2be58eac4cd8",
     "grade": false,
     "grade_id": "cell-36d48a100a6ddb8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.3 (2 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ca9ee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74715a525b6ea868aa46e9612811a809",
     "grade": true,
     "grade_id": "cell-1e34acb3833e499c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c82e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dbd040f3376e7d6a32cdcb0395dcce8",
     "grade": false,
     "grade_id": "cell-646081b13d109d59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1.4 (15 points Code+WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9275d1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7224a041d0e860f947177e01f87d6729",
     "grade": true,
     "grade_id": "cell-506e3bc2f687b05d",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74cf45f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4abc20e42d98374cefff61d298a72b58",
     "grade": false,
     "grade_id": "cell-08d1cfcc2156f6e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.2.1 (10 points WriteUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397af2d5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16fb1e938c1dc10d26a79e0b2d19dcc5",
     "grade": true,
     "grade_id": "cell-f8bcdfeff7c2b335",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d51a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01537177398acd3fc733f5ecfc3a6f9d",
     "grade": false,
     "grade_id": "cell-d502ae43d1374766",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Appendix: Neural Network Overview\n",
    "Deep learning has quickly become one of the most applied machine learning techniques in computer vision. Convolutional neural networks have been applied to many different computer vision problems such as image classification, recognition, and segmentation with great success. In this assignment, you will first implement a fully connected feed forward neural network for hand written character classification. Then in the second part, you will implement a system to locate characters in an image, which you can then classify with your deep network. The end result will be a system that, given an image of hand written text, will output the text contained in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b71b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e751aa5f73b563407f3df529cdc3eba1",
     "grade": false,
     "grade_id": "cell-4e449e493f11f2c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Basic Use\n",
    "Here we will give a brief overview of the math for a single hidden layer feed forward network. For a more detailed look at the math and derivation, please see the class slides.\n",
    "\n",
    "A fully-connected network $\\textbf{f}$, for classification, applies a series of linear and non-linear functions to an input data vector $\\textbf{x}$ of size $N\\times 1$ to produce an output vector $\\textbf{f}(\\textbf{x})$ of size $C\\times 1$, where each element $i$ of the output vector represents the probability of $\\textbf{x}$ belonging to the class $i$. Since the data samples are of dimensionality $N$, this means the input layer has $N$ input units. To compute the value of the output units, we must first compute the values of all the hidden layers. The first hidden layer *pre-activation* $\\textbf{a}^{(1)}(\\textbf{x})$ is given by\n",
    "\n",
    "$$\\textbf{a}^{(1)}(\\textbf{x}) = \\textbf{W}^{(1)}\\textbf{x} + \\textbf{b}^{(1)}$$\n",
    "\n",
    "Then the *post-activation* values of the first hidden layer $\\textbf{h}^{(1)}(\\textbf{x})$ are computed by applying a non-linear activation function $\\textbf{g}$ to the *pre-activation* values\n",
    "\n",
    "$$\\textbf{h}^{(1)}(\\textbf{x}) = \\textbf{g}(\\textbf{a}^{(1)}(\\textbf{x})) = \\textbf{g}(\\textbf{W}^{(1)}\\textbf{x} + \\textbf{b}^{(1)})$$\n",
    "\n",
    "Subsequent hidden layer ($1 < t \\leq T$) pre- and post activations are given by:\n",
    "\n",
    "$$\\textbf{a}^{(t)}(\\textbf{x}) = \\textbf{W}^{(t)}\\textbf{h}^{(t-1)} + \\textbf{b}^{(t)}$$\n",
    "\n",
    "$$\\textbf{h}^{(t)}(\\textbf{x}) = \\textbf{g}(\\textbf{a}^{(t)}(\\textbf{x}))$$\n",
    "\n",
    "The output layer *pre-activations* $\\textbf{a}^{(T)}(\\textbf{x})$ are computed in a similar way\n",
    "\n",
    "$$\\textbf{a}^{(T)}(\\textbf{x}) = \\textbf{W}^{(T)}\\textbf{h}^{(T-1)}(\\textbf{x}) + \\textbf{b}^{(T)}$$\n",
    "\n",
    "and finally the \\emph{post-activation} values of the output layer are computed with\n",
    "$$\\textbf{f}(\\textbf{x}) = \\textbf{o}(\\textbf{a}^{(T)}(\\textbf{x})) = \\textbf{o}(\\textbf{W}^{(T)}\\textbf{h}^{(T-1)}(\\textbf{x}) + \\textbf{b}^{(T)})$$\n",
    "\n",
    "where $\\textbf{o}$ is the output activation function. Please note the difference between $\\textbf{g}$ and $\\textbf{o}$! \n",
    "For this assignment, we will be using the sigmoid activation function for the hidden layer, so:\n",
    "$$\\textbf{g}(y) = \\frac{1}{1+\\exp(-y)}$$\n",
    "where when $\\textbf{g}$ is applied to a vector, it is applied element wise across the vector.\n",
    "\n",
    "Since we are using this deep network for classification, a common output activation function to use is the softmax function. This will allow us to turn the real value, possibly negative values of $\\textbf{a}^{(T)}(\\textbf{x})$ into a set of probabilities (vector of positive numbers that sum to 1). Letting $\\textbf{x}_i$ denote the $i^{th}$ element of the vector $\\textbf{x}$, the softmax function is defined as:\n",
    "$$\\textbf{o}_i(\\textbf{y}) = \\frac{\\exp(\\textbf{y}_i)}{\\sum_j \\exp(\\textbf{y}_j)}$$\n",
    "\n",
    "![](figures/letter_montage.jpg)\n",
    "<center>Samples from NIST Special 19  dataset</center>\n",
    "\n",
    "\n",
    "Gradient descent is an iterative optimisation algorithm, used to find the local optima. To find the local minima, we start at a point on the function and move in the direction of negative gradient (steepest descent) till some stopping criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a57dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "917c876a76a8dd2304785000bd206f6a",
     "grade": false,
     "grade_id": "cell-c6f5b4cd57160fca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Backprop\n",
    "The update equation for a general weight $W^{(t)}_{ij}$ and bias $b^{(t)}_i$ is\n",
    "$$\n",
    "W^{(t)}_{ij} = W^{(t)}_{ij} - \\alpha*\\frac{\\partial L_{\\textbf{f}}}{\\partial W^{(t)}_{ij}}(\\textbf{x})\\hspace{1cm}\n",
    "b^{(t)}_{i} = b^{(t)}_{i} - \\alpha*\\frac{\\partial L_{\\textbf{f}}}{\\partial b^{(t)}_{i}}(\\textbf{x})\n",
    "$$\n",
    "$\\alpha$ is the learning rate. Please refer to the back-propagation slides for more details on how to derive the gradients. Note that here we are using softmax loss (which is different from the least square loss in the slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fecfbd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec744557890bf229ea3a4f21a9b3b4e8",
     "grade": false,
     "grade_id": "cell-67800a69b8b95457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[1]  Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. 2010. http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf.\n",
    "\n",
    "[2]  P. J. Grother. Nist special database 19 – handprinted forms and characters database. https://www.nist.gov/srd/nist-special-database-19, 1995."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
